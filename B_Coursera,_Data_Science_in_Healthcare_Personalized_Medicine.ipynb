{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MahdiZakipour/from-Colaboratory/blob/main/B_Coursera%2C_Data_Science_in_Healthcare_Personalized_Medicine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17e19388",
      "metadata": {
        "id": "17e19388"
      },
      "outputs": [],
      "source": [
        "### week 1 Python data Sctructures\n",
        "my_tuple = (120, 80, 30) # Can NOT be modified\n",
        "animal_list = ['cat', 'dog', 'pp'] # Can be modified\n",
        "my_dic = {'mahdi':27, 'eli': 30} # pairs of Key:Value elemnets , \n",
        "\n",
        "# Pandas data Structures: dataframes, Series(not important, it's like dictionary)\n",
        "import pandas as pd\n",
        "my_data = {'student ID':[98745081, 98762514],\n",
        "           'name':['mahdi', 'nima'],\n",
        "           'grade': [20, 4]\n",
        "          }\n",
        "my_dataframe = pd.DataFrame(my_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e929d33",
      "metadata": {
        "id": "0e929d33",
        "outputId": "a584e241-38e6-4aef-e7c5-cba3f519d3b5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "27"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "my_tuple[1]\n",
        "animal_list[1]\n",
        "my_dic['mahdi']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a26a743e",
      "metadata": {
        "id": "a26a743e"
      },
      "outputs": [],
      "source": [
        "animal_list.append('bee')\n",
        "animal_list.remove('pp')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c67246a1",
      "metadata": {
        "id": "c67246a1",
        "outputId": "5a5f14de-859e-45e3-ff41-6373f4880de8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>student ID</th>\n",
              "      <th>name</th>\n",
              "      <th>grade</th>\n",
              "      <th>age</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>98745081</td>\n",
              "      <td>mahdi</td>\n",
              "      <td>20</td>\n",
              "      <td>27.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>98762514</td>\n",
              "      <td>nima</td>\n",
              "      <td>4</td>\n",
              "      <td>28.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   student ID   name  grade   age\n",
              "0    98745081  mahdi     20  27.0\n",
              "1    98762514   nima      4  28.5"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "animal_list[2]\n",
        "my_dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21ffc1ff",
      "metadata": {
        "id": "21ffc1ff",
        "outputId": "de2eece3-c30b-4d41-ffc0-a2a4401ecf02"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>student ID</th>\n",
              "      <th>name</th>\n",
              "      <th>grade</th>\n",
              "      <th>age</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>98745081</td>\n",
              "      <td>mahdi</td>\n",
              "      <td>20</td>\n",
              "      <td>27.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>98762514</td>\n",
              "      <td>nima</td>\n",
              "      <td>4</td>\n",
              "      <td>28.5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   student ID   name  grade   age\n",
              "0    98745081  mahdi     20  27.0\n",
              "1    98762514   nima      4  28.5"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "my_dataframe.name\n",
        "my_dataframe['age'] = [27, 28.5]\n",
        "my_dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ee37335",
      "metadata": {
        "id": "2ee37335"
      },
      "outputs": [],
      "source": [
        "#2 Reading data\n",
        "df = pd.read_csv('./readonly/physics_students.csv')\n",
        "\n",
        "df.head()\n",
        "df.tail()\n",
        "df.columns\n",
        "df.shape # rows and cols\n",
        "\n",
        "df['degree'] # a column\n",
        "df[5:10] # some observations\n",
        "df[5:10]['degree']\n",
        "df[df.age > 20]\n",
        "\n",
        "df.describe() # summarizing statisically\n",
        "df[age].mean()\n",
        "df[age].sd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a37cbf46",
      "metadata": {
        "id": "a37cbf46",
        "outputId": "46544151-2cc7-46d7-bc08-67fc40e032ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ]
        }
      ],
      "source": [
        "### WEEK 2 - Image Analysis (+ a little Genome Sequencing)\n",
        "#https://www.coursera.org/learn/datascimed/ungradedLab/B4Iyk/image-analysis-programming-task/lab?path=%2Fnotebooks%2FWK2_Image_Analysis_Task.ipynb\n",
        "\n",
        "import os\n",
        "import pydicom\n",
        "import SimpleITK\n",
        "import numpy\n",
        "import matplotlib.pyplot as plt\n",
        "%pylab inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e284dba9",
      "metadata": {
        "id": "e284dba9",
        "outputId": "a291ec9f-7da2-47d9-e33d-69182c37702f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## Part 2: Loading the data\n",
        "PathDicom = \"./readonly/MyHead/\"\n",
        "lstFilesDCM = []  # create an empty list\n",
        "for dirName, subdirList, fileList in os.walk(PathDicom):\n",
        "    for filename in fileList:\n",
        "        if \".dcm\" in filename.lower():  # check whether the file is DICOM\n",
        "            lstFilesDCM.append(os.path.join(dirName,filename))\n",
        "            \n",
        "lstFilesDCM[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9265c91",
      "metadata": {
        "id": "b9265c91",
        "outputId": "4301e648-0c88-48d6-c256-33987501cedc"
      },
      "outputs": [
        {
          "ename": "IndexError",
          "evalue": "list index out of range",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-7-4f4a16fbf7bd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Reading a DICOM file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mHeadDs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpydicom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlstFilesDCM\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Getting metadata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mHeadDs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPatientPosition\u001b[0m \u001b[1;31m# HFS stands for Head First-Supine. This means that the patient’s head was positioned toward the front of the imaging equipment and it was in an upward direction.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ],
      "source": [
        "# Reading a DICOM file\n",
        "HeadDs = pydicom.read_file(lstFilesDCM[0])\n",
        "\n",
        "# Getting metadata\n",
        "HeadDs.PatientPosition # HFS stands for Head First-Supine. This means that the patient’s head was positioned toward the front of the imaging equipment and it was in an upward direction.\n",
        "\n",
        "HeadDs.StudyDate # to get the date the study started, YYYYMMDD.\n",
        "\n",
        "HeadDs.Modality # to get the image modality, MR(MRI) | CT(CT) | PET(PT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f841872",
      "metadata": {
        "id": "3f841872"
      },
      "outputs": [],
      "source": [
        "## Part 3: Visualisation\n",
        "# Preparing for visualisation, In order to plot the data with Matplotlib, FIRST: combine the pixel data from all DICOM files (i.e. from all slices) into a 3D dataset\n",
        "CalcPixelDims = (int(HeadDs.Rows), int(HeadDs.Columns), len(lstFilesDCM))\n",
        "CalcPixelDims\n",
        "\n",
        "HeadImgArray = numpy.zeros(CalcPixelDims, dtype=HeadDs.pixel_array.dtype)\n",
        "\n",
        "for filenameDCM in lstFilesDCM:\n",
        "    ds = pydicom.read_file(filenameDCM)\n",
        "    HeadImgArray[:, :, lstFilesDCM.index(filenameDCM)] = ds.pixel_array\n",
        "    \n",
        "# SECOND : specify appropriate coordinate axes\n",
        "CalcPixelSpacing = (float(HeadDs.PixelSpacing[0]), float(HeadDs.PixelSpacing[1]), float(HeadDs.SliceThickness))\n",
        "\n",
        "x = numpy.arange(0.0, (CalcPixelDims[0]+1)*CalcPixelSpacing[0], CalcPixelSpacing[0])\n",
        "y = numpy.arange(0.0, (CalcPixelDims[1]+1)*CalcPixelSpacing[1], CalcPixelSpacing[1])\n",
        "z = numpy.arange(0.0, (CalcPixelDims[2]+1)*CalcPixelSpacing[2], CalcPixelSpacing[2])\n",
        "\n",
        "# visulizing\n",
        "plt.figure(dpi=300)\n",
        "plt.axes().set_aspect('equal', 'datalim')\n",
        "plt.set_cmap(plt.gray())\n",
        "plt.pcolormesh(x, y, numpy.flipud(HeadImgArray[:, :, 125]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66ea7ea6",
      "metadata": {
        "id": "66ea7ea6"
      },
      "outputs": [],
      "source": [
        "### Specifying a helper function, that quickly plots a 2D SimpleITK image with a greyscale colourmap and accompanying axes.\n",
        "def sitk_show(img, title=None, margin=0.05, dpi=40 ):\n",
        "    nda = SimpleITK.GetArrayFromImage(img)\n",
        "    spacing = img.GetSpacing()\n",
        "    figsize = (1 + margin) * nda.shape[0] / dpi, (1 + margin) * nda.shape[1] / dpi\n",
        "    extent = (0, nda.shape[1]*spacing[1], nda.shape[0]*spacing[0], 0)\n",
        "    fig = plt.figure(figsize=figsize, dpi=dpi)\n",
        "    ax = fig.add_axes([margin, margin, 1 - 2*margin, 1 - 2*margin])\n",
        "\n",
        "    plt.set_cmap(\"gray\")\n",
        "    ax.imshow(nda,extent=extent,interpolation=None)\n",
        "    \n",
        "    if title:\n",
        "        plt.title(title)\n",
        "    \n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c29801b",
      "metadata": {
        "id": "6c29801b"
      },
      "outputs": [],
      "source": [
        "### Loading the data in SimpleITK\n",
        "reader = SimpleITK.ImageSeriesReader()\n",
        "filenamesDICOM = reader.GetGDCMSeriesFileNames(PathDicom)\n",
        "reader.SetFileNames(filenamesDICOM)\n",
        "img3DOriginal = reader.Execute()\n",
        "\n",
        "# for simplicity, we'll segment a 2D slice of the 3D image (rather than the entire 3D image)\n",
        "imgOriginal = img3DOriginal[:,:,50] \n",
        "\n",
        "### Visualising the original data\n",
        "sitk_show(imgOriginal)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c5ddc95",
      "metadata": {
        "id": "7c5ddc95"
      },
      "outputs": [],
      "source": [
        "### Smoothing,  reducing noise within an image or producing a less pixelated image\n",
        "imgSmooth = SimpleITK.CurvatureFlow(image1=imgOriginal,\n",
        "                                    timeStep=0.125,\n",
        "                                    numberOfIterations=5)\n",
        "\n",
        "sitk_show(imgSmooth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8713bf7",
      "metadata": {
        "id": "e8713bf7"
      },
      "outputs": [],
      "source": [
        "### Segmentation with the ConnectedThreshold filter\n",
        "lstSeeds = [(150,75)] # the starting point, which we know is e.g. white matter.\n",
        "\n",
        "imgWhiteMatter = SimpleITK.ConnectedThreshold(image1=imgSmooth, \n",
        "                                              seedList=lstSeeds, \n",
        "                                              lower=130, \n",
        "                                              upper=190,\n",
        "                                              replaceValue=1)\n",
        "\n",
        "# preprocess, overlay, and visualizing the result\n",
        "imgSmoothInt = SimpleITK.Cast(SimpleITK.RescaleIntensity(imgSmooth), imgWhiteMatter.GetPixelID())\n",
        "\n",
        "sitk_show(SimpleITK.LabelOverlay(imgSmoothInt, imgWhiteMatter))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afa27f1f",
      "metadata": {
        "id": "afa27f1f"
      },
      "outputs": [],
      "source": [
        "#### Hole-filling of the segmented white matter\n",
        "imgWhiteMatterNoHoles = SimpleITK.VotingBinaryHoleFilling(image1=imgWhiteMatter,\n",
        "                                                          radius=[2]*3,\n",
        "                                                          majorityThreshold=1,\n",
        "                                                          backgroundValue=0,\n",
        "                                                          foregroundValue=1)\n",
        "\n",
        "sitk_show(SimpleITK.LabelOverlay(imgSmoothInt, imgWhiteMatterNoHoles))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34275c4e",
      "metadata": {
        "id": "34275c4e"
      },
      "outputs": [],
      "source": [
        "#### Segmentation and hole-filling of grey matter\n",
        "# we just repeat the whole above process for grey matter parts...\n",
        "lstSeeds = [(119, 83), (198, 80), (185, 102), (164, 43)]\n",
        "\n",
        "imgGreyMatter = SimpleITK.ConnectedThreshold(image1=imgSmooth, \n",
        "                                             seedList=lstSeeds, \n",
        "                                             lower=150, \n",
        "                                             upper=270,\n",
        "                                             replaceValue=2)\n",
        "\n",
        "imgGreyMatterNoHoles = SimpleITK.VotingBinaryHoleFilling(image1=imgGreyMatter,\n",
        "                                                         radius=[2]*3,\n",
        "                                                         majorityThreshold=1,\n",
        "                                                         backgroundValue=0,\n",
        "                                                         foregroundValue=2) # labelGrayMatter\n",
        "\n",
        "sitk_show(SimpleITK.LabelOverlay(imgSmoothInt, imgGreyMatterNoHoles))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "902b4d7b",
      "metadata": {
        "id": "902b4d7b"
      },
      "outputs": [],
      "source": [
        "#### Combining the white and grey matter (combining the 2 label fields)\n",
        "imgLabels = imgWhiteMatterNoHoles | imgGreyMatterNoHoles\n",
        "\n",
        "sitk_show(SimpleITK.LabelOverlay(imgSmoothInt, imgLabels))\n",
        "\n",
        "imgMask = (imgWhiteMatterNoHoles/1) * (imgGreyMatterNoHoles/2)\n",
        "imgMask2 = SimpleITK.Cast(imgMask, imgWhiteMatterNoHoles.GetPixelIDValue())\n",
        "imgWhiteMatterNoHoles = imgWhiteMatterNoHoles - (imgMask2*1)\n",
        "imgLabels2 = imgWhiteMatterNoHoles + imgGreyMatterNoHoles\n",
        "\n",
        "sitk_show(SimpleITK.LabelOverlay(imgSmoothInt, imgLabels2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7df1263c",
      "metadata": {
        "id": "7df1263c",
        "outputId": "35d19960-8ba5-4d21-cf42-57765043f0e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "key of iris dataset ARE: \n",
            " dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename'])\n",
            ".. _iris_dataset:\n",
            "\n",
            "Iris plants dataset\n",
            "--------------------\n",
            "\n",
            "**Data Set Characteristics:**\n",
            "\n",
            "    :Number of Instances: 150 (50 in each of three classes)\n",
            "    :Number of Attributes: 4 numeric, predictive\n",
            ".......\n",
            "Feature names ARE: \n",
            " ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
            "Target names ARE:  ['setosa' 'versicolor' 'virginica']\n",
            "Shape of target IS:  (150,)\n",
            "First two elements in target ARE:  [0 0]\n",
            "The shape of data IS:  (150, 4)\n",
            "First three rows of data ARE:\n",
            " [[5.1 3.5 1.4 0.2]\n",
            " [4.9 3.  1.4 0.2]\n",
            " [4.7 3.2 1.3 0.2]]\n"
          ]
        }
      ],
      "source": [
        "### week 3 - Machine Learning\n",
        "import sklearn\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "iris_dataset = load_iris()\n",
        "\n",
        "print(\"key of iris dataset ARE: \\n\", iris_dataset.keys()) # inspecting data\n",
        "print(iris_dataset['DESCR'][:200] + \"\\n.......\")\n",
        "print(\"Feature names ARE: \\n\", iris_dataset['feature_names'])\n",
        "print(\"Target names ARE: \", iris_dataset['target_names']) # target_names = the class labels\n",
        "print(\"Shape of target IS: \", iris_dataset['target'].shape)\n",
        "print(\"First two elements in target ARE: \", iris_dataset['target'][:2])\n",
        "\n",
        "print(\"The shape of data IS: \", iris_dataset['data'].shape)\n",
        "print(\"First three rows of data ARE:\\n\", iris_dataset['data'][:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "304dce51",
      "metadata": {
        "id": "304dce51",
        "outputId": "cb6bbb4d-b6ef-4eea-ff9b-75e6b6197bd0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train shape:  (112, 4)\n",
            "y_train shape:  (112,)\n",
            "X_test shape:  (38, 4)\n",
            "y_test shape:  (38,)\n"
          ]
        }
      ],
      "source": [
        "# Splitting our dataset into training  and test \n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    iris_dataset['data'], iris_dataset['target'], random_state=0)\n",
        "\n",
        "print(\"X_train shape: \", X_train.shape)\n",
        "print(\"y_train shape: \", y_train.shape)\n",
        "print(\"X_test shape: \", X_test.shape)\n",
        "print(\"y_test shape: \", y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f48626b",
      "metadata": {
        "id": "0f48626b",
        "outputId": "83c7deea-cdbd-4425-e236-84f8574bcdc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test set score:  0.9736842105263158\n",
            "Test set score rounded to three decimal places: 0.974\n",
            "\n",
            " Prediction label:  [0]\n",
            "\n",
            " Predicted target name:  ['setosa']\n"
          ]
        }
      ],
      "source": [
        "# Our first model: KNN - K Nearest Neighbours\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knn = KNeighborsClassifier(n_neighbors=1)\n",
        "\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Evaluating the model \n",
        "print(\"Test set score: \", knn.score(X_test, y_test)) \n",
        "print(\"Test set score rounded to three decimal places: {:.3f}\".format(knn.score(X_test, y_test)))\n",
        "\n",
        "# Make Predictions\n",
        "import numpy as np\n",
        "X_unseen = np.array([[5.3, 2.7, 1, 0.3]])\n",
        "\n",
        "prediction = knn.predict(X_unseen)\n",
        "\n",
        "print(\"\\n Prediction label: \", prediction)\n",
        "print(\"\\n Predicted target name: \", iris_dataset['target_names'][prediction])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf201ec9",
      "metadata": {
        "id": "cf201ec9",
        "outputId": "0f681dfa-3eba-46de-dd0d-d6e1c75891b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on training set:  1.0\n",
            "Accuracy on test set:  0.8947368421052632\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    iris_dataset['data'], iris_dataset['target'], random_state=7)\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "tree = DecisionTreeClassifier(random_state=12)\n",
        "tree.fit(X_train, y_train)\n",
        "\n",
        "print(\"Accuracy on training set: \", tree.score(X_train, y_train))\n",
        "print(\"Accuracy on test set: \", tree.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "199bd0ac",
      "metadata": {
        "id": "199bd0ac",
        "outputId": "b5df6061-3404-43c6-95ca-6f7389c4ddcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy on training set:  0.9910714285714286\n",
            "Accuracy on test set:  0.9210526315789473\n"
          ]
        }
      ],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "tree = DecisionTreeClassifier(max_depth=3, random_state=12)\n",
        "tree.fit(X_train, y_train)\n",
        "\n",
        "print(\"Accuracy on training set: \", tree.score(X_train, y_train))\n",
        "print(\"Accuracy on test set: \", tree.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3865e91",
      "metadata": {
        "id": "f3865e91",
        "outputId": "4abd8e22-013a-4481-ed5a-6c720ae5d689"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction label:  [0]\n",
            "Predicted target name:  ['setosa']\n"
          ]
        }
      ],
      "source": [
        "prediction = tree.predict(X_unseen)\n",
        "\n",
        "print(\"Prediction label: \", prediction)\n",
        "print(\"Predicted target name: \", iris_dataset['target_names'][prediction])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5ce3446",
      "metadata": {
        "id": "a5ce3446"
      },
      "outputs": [],
      "source": [
        "### WEEK 3 - Machine Learning; Programming Assignmet \n",
        "#https://www.coursera.org/learn/datascimed/ungradedLab/Ae3kz/programming-assignment-notebook/lab?path=%2Fnotebooks%2FProgramming_Assignment.ipynb\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec7c2fbe",
      "metadata": {
        "id": "ec7c2fbe"
      },
      "outputs": [],
      "source": [
        "### WEEK 4 - NLP, Natural Language Processing\n",
        "import nltk\n",
        "import docx2txt\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96b51390",
      "metadata": {
        "id": "96b51390",
        "outputId": "936b50af-9459-4ac6-b8c0-7c7084f7c28c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to C:\\Users\\همه\n",
            "[nltk_data]     مون\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to C:\\Users\\همه\n",
            "[nltk_data]     مون\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\همه مون\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to C:\\Users\\همه\n",
            "[nltk_data]     مون\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to C:\\Users\\همه\n",
            "[nltk_data]     مون\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d407daf7",
      "metadata": {
        "id": "d407daf7",
        "outputId": "97580cf4-07b9-49b7-dd31-8d6f76713ce4"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './readonly/Biopsy_Report.docx'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-8-98d8e6457e0a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m## Processing a Biopsy Report\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Loading the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdocx2txt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./readonly/Biopsy_Report.docx'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#using the process method from the docx2txt Python package to convert from .docx into Plain_Text.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m160\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# to get the first 160 characters in text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mD:\\Softwares\\AnacondaSetUp\\envs\\Mahdi\\lib\\site-packages\\docx2txt\\docx2txt.py\u001b[0m in \u001b[0;36mprocess\u001b[1;34m(docx, img_dir)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[1;31m# unzip the docx in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m     \u001b[0mzipf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m     \u001b[0mfilelist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzipf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnamelist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mD:\\Softwares\\AnacondaSetUp\\envs\\Mahdi\\lib\\zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[0;32m   1249\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1250\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1251\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1252\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1253\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mfilemode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './readonly/Biopsy_Report.docx'"
          ]
        }
      ],
      "source": [
        "## Processing a Biopsy Report\n",
        "# Loading the data\n",
        "text = docx2txt.process('./readonly/Biopsy_Report.docx') #using the process method from the docx2txt Python package to convert from .docx into Plain_Text.\n",
        "type(text)\n",
        "text[:160] # to get the first 160 characters in text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cc7e073",
      "metadata": {
        "id": "4cc7e073"
      },
      "outputs": [],
      "source": [
        "## Tokenization\n",
        "tokens = nltk.word_tokenize(text) # into Words !\n",
        "tokens[:10] # to get the first 10 elements of tokens\n",
        "\n",
        "# Cleaning, (i.e. removing the ANDs, ORs, AREs, etc...)\n",
        "clean_tokens = tokens[:]\n",
        "for token in tokens:\n",
        "    if token in stopwords.words('english'):\n",
        "        clean_tokens.remove(token)\n",
        "\n",
        "print(\"Number of tokens including stop words:  \",len(tokens))\n",
        "print(\"Number of tokens excluding stop words:  \",len(clean_tokens))\n",
        "\n",
        "# Frequency Distribution of some certain Words\n",
        "freq = nltk.FreqDist(tokens)\n",
        "freq.most_common(10) # the most commons\n",
        "\n",
        "print(\"Frequency of lesion:  \", freq[\"lesion\"]) # freq of \"lesion\"\n",
        "print(\"Frequency of lesions: \", freq[\"lesions\"])\n",
        "print(\"Frequency of LESION:  \", freq[\"LESION\"])\n",
        "print(\"Frequency of LESIONS: \", freq[\"LESIONS\"])\n",
        "\n",
        "# Lower case vs. upper case text\n",
        "lowercase_tokens = [t.lower() for t in tokens]\n",
        "lowercase_tokens[:10]\n",
        "\n",
        "lowercase_freq = nltk.FreqDist(lowercase_tokens)\n",
        "print(\"Frequency of lesion:  \", lowercase_freq[\"lesion\"]) # freq of \"lesion\"\n",
        "print(\"Frequency of lesions: \", lowercase_freq[\"lesions\"])\n",
        "print(\"Frequency of LESION:  \", lowercase_freq[\"LESION\"])\n",
        "print(\"Frequency of LESIONS: \", lowercase_freq[\"LESIONS\"])\n",
        "\n",
        "# Stemming, the process of reducing a word to its stem.\n",
        "stemmer = nltk.PorterStemmer()\n",
        "stem_tokens = lowercase_tokens\n",
        "stem_tokens[:] = [stemmer.stem(lt) for lt in lowercase_tokens]\n",
        "\n",
        "stem_freq = nltk.FreqDist(stem_tokens)\n",
        "print(\"Frequency of lesion:  \", stem_freq[\"lesion\"]) # freq of \"lesion\"\n",
        "print(\"Frequency of lesions: \", stem_freq[\"lesions\"])\n",
        "print(\"Frequency of LESION:  \", stem_freq[\"LESION\"])\n",
        "print(\"Frequency of LESIONS: \", stem_freq[\"LESIONS\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ebe6f17",
      "metadata": {
        "id": "5ebe6f17"
      },
      "outputs": [],
      "source": [
        "## Processing a Medical Note\n",
        "# Load the Data\n",
        "content = docx2txt.process('./readonly/Medical_Note.docx')\n",
        "content[:160]\n",
        "\n",
        "# Tokenisation\n",
        "sents = nltk.sent_tokenize(content) # into Sentences !\n",
        "sents[:4]\n",
        "\n",
        "# now, just the Sent[1] for further processing\n",
        "medical_tokens = nltk.word_tokenize(sents[1])\n",
        "medical_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "969250b8",
      "metadata": {
        "id": "969250b8"
      },
      "outputs": [],
      "source": [
        "## Part-of-Speech Tagging, processes a sequence of words and attaches a part of speech tag to each word\n",
        "# meaning that, we'll know which part is Adjective OR Noun OR Verb, etc ...\n",
        "tagged = nltk.pos_tag(medical_tokens)\n",
        "tagged"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b3718be",
      "metadata": {
        "id": "0b3718be",
        "outputId": "23a32d6b-0d07-435a-984f-f6c85c8c65c7"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'tagged' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-11-e9299a147d7b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m## Named Entity Recognition, finding entities in text & classifying them as persons, locations, date,...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mentities\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mne_chunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtagged\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'tagged' is not defined"
          ]
        }
      ],
      "source": [
        "## Named Entity Recognition, finding entities in text & classifying them as persons, locations, date,...\n",
        "entities = nltk.ne_chunk(tagged)\n",
        "print(entities)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afb93a1f",
      "metadata": {
        "id": "afb93a1f"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "B - Coursera, Data Science in Healthcare Personalized Medicine.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}